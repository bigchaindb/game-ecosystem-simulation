'use strict';

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.default = computeFileHash;

var _sparkMd = require('spark-md5');

var _sparkMd2 = _interopRequireDefault(_sparkMd);

var _safe_invoke = require('../safe_invoke');

var _safe_invoke2 = _interopRequireDefault(_safe_invoke);

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

/**
 * Takes a file, computes the MD5 hash and returns a promise that will resolve with the hash.
 * Adapted from https://github.com/satazor/js-spark-md5
 *
 * @param  {File}     file         Javascript File object
 * @param  {function} [onProgress] Progress callback that will be invoked with the current progress
 *                                 (as a percent) after every chunk is read. Explicitly returning
 *                                 `false` from this callback will stop and cancel the computation.
 *   @param  {number}  progress Progress as a percentage.
 *   @return {boolean}          Return `false` to stop and cancel the computation
 *
 * @param  {object}   [options]    Options for computing file hash
 * @param  {number}   options.chunkSize Size of chunks to read file
 *
 * @return {Promise}               Promise that resolves with the hash of the file as a string
 */
function computeFileHash(file, onProgress, options) {
    return new Promise(function (resolve, reject) {
        var blobSlice = File.prototype.slice || File.prototype.mozSlice || File.prototype.webkitSlice;
        var chunkSize = options && options.chunkSize || 2097152; // By default, read in chunks of 2MB
        var chunks = Math.ceil(file.size / chunkSize);
        var spark = new _sparkMd2.default.ArrayBuffer();
        var fileReader = new FileReader();
        var startTime = new Date();
        var currentChunk = 0;

        fileReader.onload = function (_ref) {
            var result = _ref.target.result;

            if (process.env.NODE_ENV !== 'production') {
                // eslint-disable-next-line no-console
                console.info('Hashing chunk ' + (currentChunk + 1) + ' of ' + chunks);
            }

            spark.append(result); // Append array buffer
            currentChunk++;

            if (currentChunk < chunks) {
                loadNext();
            } else {
                var fileHash = spark.end();

                if (process.env.NODE_ENV !== 'production') {
                    var hashTime = (new Date() - startTime) / 1000 % 60;

                    /* eslint-disable no-console */
                    console.info('Computing hash of ' + file.name + ' took ' + hashTime + 's');
                    console.info('Computed hash: ' + fileHash);
                    /* eslint-enable no-console */
                }

                resolve(fileHash);
            }
        };

        fileReader.onerror = reject;

        function loadNext() {
            var start = currentChunk * chunkSize;
            var nextEnd = start + chunkSize;
            var end = nextEnd >= file.size ? file.size : nextEnd;

            var _safeInvoke = (0, _safe_invoke2.default)(onProgress, start / file.size * 100);

            var shouldProgress = _safeInvoke.result;


            if (shouldProgress !== false) {
                fileReader.readAsArrayBuffer(blobSlice.call(file, start, end));
            } else {
                reject(new Error(file.name ? 'Hashing cancelled for ' + file.name : 'Hashing cancelled'));
            }
        }

        loadNext();
    });
}
module.exports = exports['default'];